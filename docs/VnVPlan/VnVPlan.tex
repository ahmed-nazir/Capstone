\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{float}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Project Title: System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Developer} & {\bf Notes}\\
\midrule
October 30 & Stephen & Added General Information and Plan\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations or acronyms -- you can simply reference the SRS
  \citep{SRS} tables, if appropriate}

\newpage

\pagenumbering{arabic}

This document ... \wss{provide an introductory blurb and roadmap of the
  Verification and Validation plan}

\section{General Information}

\subsection{Summary}

\wss{Say what software is being tested.  Give its name and a brief overview of
  its general functions.}

  Formulate consists of four subsystems, one hardware subsystem and three software subsystems, which interact to provide the user with a testing device designed to eliminate automatable processes in common testing procedures.\\

  A physical data collection device is the hardware subsystem used as the first point of contact with the measured quantity through a sensor. The sensor obtains physical quantities for the device to buffer, before sending the data to a desktop application for user verification.\\

  The user has the ability to view the data collected by the physical device after a completed test using a desktop application software subsystem. The desktop application enables the user to either accept the test results to then store a collection of data from a test to a database, or reject the test and prevent the test from being stored to a database.\\

  Accepted test data sent from the desktop application aggregates and saves verified test results to a database software subsystem. Users can query the database to obtain common statistics in test data and generate new or obscure relationships by leveraging database language capabilities.\\

  A final dashboard software subsystem then queries the database to visualize key performance indicators on the test data collected and stored in the database.\newpage





\subsection{Objectives}

\wss{State what is intended to be accomplished.  The objective will be around
  the qualities that are most important for your project.  You might have
  something like: ``build confidence in the software correctness,''
  ``demonstrate adequate usability.'' etc.  You won't list all of the qualities,
  just those that are most important.}

  The objective of the system Verification and Validation (VnV) plan for Formulate is to ensure the intended project qualities are present.\\

  Ease in user understanding is a quality Formulate will achieve to support the system's usability.  Specifically, ease in user understanding of each subsystem's function and how subsystems interface will be key qualities of the overall project.\\

  The system will also demonstrate the quality of adequate portability and physical robustness to support system maintainability, portability, and operationality.\\



\subsection{Relevant Documentation}

\wss{Reference relevant documentation.  This will definitely include your SRS
  and your other project documents (MG, MIS, etc).  You can include these even
  before they are written, since by the time the project is done, they will be
  written.}

  Talk about how this document draws from the system requirements gathered during software requirements specification (SRS) and hazard analysis.\\

  This document references a variety of requirements generated during the Software Requirements Specification (SRS) process and the Hazard Analysis (HA) process for the Formulate system. \\


\citet{SRS}
\newpage
\section{Plan}

\wss{Introduce this section.   You can provide a roadmap of the sections to
  come.}
  
  \subsection{Roadmap}

  The intention of testing for Formulate is to generate confidence that the project meets qualities relating to usability, maintainability, portability, operationality, and safety set out as requirements in SRS and HA documentation. Through sets of unit and system tests that will prove if the system has met the above requirements, Formulate will understand if the project has achieved the desired qualities.\\

  Specifically, requirements that are functional, non-functional, and safety-security related from the SRS and HA documents will be referenced in the Plan, System Test Description, and Unit Test Description sections of this document.\\

\subsection{Verification and Validation Team}

\wss{You, your classmates and the course instructor.  Maybe your supervisor.
  You shoud do more than list names.  You should say what each person's role is
  for the project.  A table is a good way to summarize this information.}

  \begin{table}[H]
    \centering
    \begin{tabular}{|p{3cm}|p{4cm}|p{7cm}|}
    \hline
    \multicolumn{1}{|c|}{\textbf{Name}} & \multicolumn{1}{|c|}{\textbf{Role}} & \multicolumn{1}{|c|}{\textbf{Explanation}}
    \\ \hline
    Stephen
    & Desktop Application Tester
    & VnV for software application design and integration with hardware and database
    \newline                                
    \\ \hline
  
    Ahmed                              
    & Hardware Device Tester
    & VnV for embedded program design, chassis design, and integration with desktop application
    \newline                                
    \\ \hline
  
    Muhanad                          
    & Database Application Tester
    & VnV for database design and integration with desktop application and dashboard
    \newline                                
    \\ \hline
  
    Tioluwalayomi                                
    & Dashboard Application Tester
    & VnV for dashboard application design and integration with database
    \newline                            
    \\ \hline

    Timofey                                
    & Project and Course Teaching Assistant
    & Detailed low level feedback on planned VnV tests
    \newline                            
    \\ \hline
  
    Dr. Smith                                
    & Course Instructor
    & General high level feedback on planned VnV tests 
    \newline                            
    \\ \hline
  
    \end{tabular}
  \end{table}
  \newpage

\subsection{SRS Verification Plan}

\wss{List any approaches you intend to use for SRS verification.  This may just
  be ad hoc feedback from reviewers, like your classmates, or you may have
  something more rigorous/systematic in mind..}

\wss{Remember you have an SRS checklist}

SRS verification will be composed of two approaches to verify that functional and non-functional requirements are met. The first approach is engaging in read through's of the SRS document each month. Individual member progress will be evaluated against the relevenat sections(s) of the SRS document to ensure system  development is on track to meet the requirements. The second approach is evaluating issues created by classmates on GitHub and incorporating their concerns and suggestions as seen fit.\\

Stephen and Tioluwalayomi will lead the group wide discussion for SRS verification activities on the first Tuesday of each month.\\

\subsection{Design Verification Plan}

\wss{Plans for design verification}

\wss{The review will include reviews by your classmates}

\wss{Remember you have MG and MIS checklists}

Design verification will be composed of two approaches. The first planned approach is completing read through's of each individual's high level design documentation for their respective sub-system. The design documentation covered during these read through's will likely entail mechanical, electrical, and or software schematics or diagrams outlining the architecture of the subsystem. Members will voice concerns during the read through of design decisions made by the individual responsible for the sub-system architecture. The second planned approach is evaluating issues created by classmates on GitHub and incorporating their concerns and suggestions as seen fit.\\

Ahmed will lead the group wide discussion for design verification activities on the second Tuesday of each month.\newpage

\subsection{Implementation Verification Plan}

\wss{You should at least point to the tests listed in this document and the unit
  testing plan.}

\wss{In this section you would also give any details of any plans for static verification of
  the implementation.  Potential techniques include code walkthroughs, code
  inspection, static analyzers, etc.}

  Implementation verification will be composed of techniques in both static and dynamic analysis. \\

  Content walkthrough is the primary type of static implementation verification technique the group plans on using. Three similar types of content walkthrough's are planned for use depending on the sub-system under analysis. Software implementation's will receive a code walkthrough, mechanical implementation's will receive a Computer Aided Design (CAD) spin, and electrical implementation's will receive a schematic walkthrough. During the content walkthrough, unit and system tests relevant to the implementation will be considered to critique the quality of the implementation. A meeting will be organized for each content walkthrough once the implementation has reached a notable milestone worthwile for group analysis.\\
  
  Live execution of the implementation using a proof of concept style demonstration to the group is the primary type of dynamic implementation technique the group plans on using. During the live demonstration of the implementation, system and unit tests relevant to the implementation will be considered to critique the quality of the implementation. Using the initial state and inputs of the test outlined in the system and unit test sections, the quality of implementation is passed or failed depending on if the actual output of the implementation matches the expeted output.\\

Ahmed and Muhanad will lead the group wide discussion for design verification activities on the second Tuesday of each month.\newpage

\subsection{Automated Testing and Verification Tools}

\wss{What tools are you using for automated testing.  Likely a unit testing
  framework and maybe a profiling tool, like ValGrind.  Other possible tools
  include a static analyzer, make, continuous integration tools, test coverage
  tools, etc.  Explain your plans for summarizing code coverage metrics.
  Linters are another important class of tools.  For the programming language
  you select, you should look at the available linters.  There may also be tools
  that verify that coding standards have been respected, like flake9 for
  Python.}

  Tools will not be used to automate testing, profile, or code coverage for software flashed on embedded hardware because of the available tools incur high additional overhead to testing effort and times. The group plans on completing extensive unit testing for embedded software to compensate for the absence of automated testing and coverage tools.\\

  The desktop application will likely use Visual Studio's memory usage tool to profile the program during execution.\\

  PyLint and SQLFluff will be used as the static code analysis tools to support uniformity in the desktop application and database programs respectively.\\

  Code coverage will be completed manually for the desktop, database, and the dashboard programs. This decision will be feasible as the expected size of software for the applications listed above is relatively small. As a result, it is reasonable for to manually check the amount of code coverage achieved through tests.\\

\wss{The details of this section will likely evolve as you get closer to the
  implementation.}

\subsection{Software Validation Plan}

\wss{If there is any external data that can be used for validation, you should
  point to it here.  If there are no plans for validation, you should state that
  here.}

  There are no current plans to use external data for validation.\newpage

\section{System Test Description}
	
\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into
  different areas.  If there are no identifiable subsets for the tests, this
  level of document structure can be removed.}

\wss{Include a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good.}

\subsubsection{Hardware Device}

\wss{It would be nice to have a blurb here to explain why the subsections below
  cover the requirements.  References to the SRS would be good.  If a section
  covers tests for input constraints, you should reference the data constraints
  table in the SRS.}
		
\paragraph{Sensor Validation}

\begin{enumerate}

\item{\bf{ST-SV 1}}

Type: Manual
					
Initial State: Device is on measuring and sending values to the application, and connection to database has been verified
					
Input: The device is placed in temperature controlled box at 25$^{\circ}$C
					
Output: The temperature sensor on the device should consistently read 25$^{\circ}$C

%Test Case Derivation: \wss{Justify the expected value given in the Output field}
					
How test will be performed: The device will be placed in a styrofoam insulated box and a hair dryer will be used to heat the box. Using a digital thermometer we will compare the results of our device to the digital thermometer\\

		
\item{\bf{ST-SV 2}}

Type: Manual
					
Initial State: Device is on measuring and sending values to the application, and connection to database has been verified
					
Input: The device will be placed in an humidity controlled box which will be around 40\% humidity
					
Output: The humidity sensor on the device should consistently read 40\% humidity

How test will be performed: The device will be placed in a styrofoam insulated box and a hair dryer will be used to heat the box. Using a digital humidity monitor we will compare the results of our device to the digital sensor

\end{enumerate}

\paragraph{Device Telemetry}	
\begin{enumerate}

  \item{\bf{ST-DT 1}}
  
  Type: Manual
            
  Initial State: Device is on and is connected to a PC via cable or via Wi-Fi
            
  Input: The start button is pressed
            
  Output: All the readings from all the different sensors should be sent to the PC
  
  %Test Case Derivation: \wss{Justify the expected value given in the Output field}
            
  How test will be performed: While the device is connected to the PC the start button will be pressed to check if telemetry data is being sent to the PC\\

  \item{\bf{ST-DT 2}}
  
  Type: Manual
            
  Initial State: Device is on and is connected to a PC via cable or via Wi-Fi
            
  Input: The start button is pressed multiple times
            
  Output: The device should continue to send data to the PC regardless of how many times the start button is pressed
  
  %Test Case Derivation: \wss{Justify the expected value given in the Output field}
            
  How test will be performed: While the device is connected to the PC the start button will be pressed and then after a few seconds pressed again 2 more times to check if telemetry data is still being sent to the PC\\

  \item{\bf{ST-DT 3}}
  
  Type: Manual
            
  Initial State: Device is on and is connected to a PC via cable or via Wi-Fi
            
  Input: The stop button is pressed
            
  Output: All the readings from all the different sensors should stop being sent to the PC
  
  %Test Case Derivation: \wss{Justify the expected value given in the Output field}
            
  How test will be performed: While the device is connected to the PC the stop button will be pressed to check if telemetry data is stopped\\

  \item{\bf{ST-DT 4}}
  
  Type: Manual
            
  Initial State: Device is on and is connected to a PC via cable or via Wi-Fi
            
  Input: The stop button is pressed multiple times
            
  Output: The device should not send data to the PC regardless of how many times the stop button is pressed
  
  %Test Case Derivation: \wss{Justify the expected value given in the Output field}
            
  How test will be performed: While the device is connected to the PC the stop button will be pressed in an interval of 2 seconds to check if telemetry data is has stopped\\

  \end{enumerate}

  \paragraph{Device Hardware}	
  \begin{enumerate}
  
    \item{\bf{ST-DH 1}}
    
    Type: Manual
              
    Initial State: Device is on and is connected to a PC, no sensors are attached
              
    Input: A sensor is connected and start is pressed
              
    Output: The device should start sending data to the PC
    
              
    How test will be performed: While the device is connected to the PC a sensor is connected to the modular port on the device and the start button is pressed\\
  
    \item{\bf{ST-DH 2}}
    
    Type: Manual
              
    Initial State: Device is on but is low on battery
              
    Input: User plugs the battery in with a charger
              
    Output: The battery should begin to charge
    
    %Test Case Derivation: \wss{Justify the expected value given in the Output field}
              
    How test will be performed: We will continue to use the device until the battery is completely drained, once its low we will start to charge it and check if the battery increases\\

    \item{\bf{ST-DH 3}}
    
    Type: Manual
              
    Initial State: Device is fully mounted on a test platform
              
    Input: User will apply 49N of force on every side of the device
              
    Output: The device should not move at all
    
    %Test Case Derivation: \wss{Justify the expected value given in the Output field}
              
    How test will be performed: Once the device is mounted we will place 5kg on top of different sides of the device to check if the mount can withstand it \\

    \item{\bf{ST-DH 4}}
    
    Type: Manual
              
    Initial State: Device is not mounted on anything
              
    Input: User will begin to mount the device on the Formula SAE car
              
    Output: The device should be fully installed under 5 minutes
    
    %Test Case Derivation: \wss{Justify the expected value given in the Output field}
              
    How test will be performed: While someone is mounting the device another person is timing them \\
  
    \end{enumerate}


\subsubsection{Desktop Application}

\begin{enumerate}
  
  \item{\bf{ST-DA 1}}
  
  Type: Manual
            
  Initial State: Device is on and is connected to a PC, the desktop application is open and connected to the device
            
  Input: User selects the most recent test case
            
  Output: A table with all the raw test data should populate the screen
  
  %Test Case Derivation: \wss{Justify the expected value given in the Output field}
            
  How test will be performed: After a test is completed on the device, it will be connected to the PC and the user should be able to view the most recent test after clicking view\\

  \item{\bf{ST-DA 2}}
  
  Type: Manual
            
  Initial State: Device is on and is connected to a PC, the desktop application is open and test data is open
            
  Input: User selects rows and columns to delete and hits the delete key
            
  Output: The table with the test data should update to reflect the newly deleted rows
  
  %Test Case Derivation: \wss{Justify the expected value given in the Output field}
            
  How test will be performed: A test will be conducted but the start button will be pressed early, the initial 20 seconds of the test does not provide useful data and the user will select and delete those rows\\

  \item{\bf{ST-DA 3}}
  
  Type: Manual
            
  Initial State: Device is on and is connected to a PC, the desktop application is open and test data is open
            
  Input: User hits the submit to database button
            
  Output: The test data in the table should be sent to the database
  
  %Test Case Derivation: \wss{Justify the expected value given in the Output field}
            
  How test will be performed: After a test is conducted and the data will be viewed on the desktop app, the user will preview the data and click submit to database \\

  \item{\bf{ST-DA 4}}
  
  Type: Manual
            
  Initial State: Device is on and is connected to a PC, the desktop application is open but no tests are done
            
  Input: User hits the submit to database button
            
  Output: The application should alert the user that no data was recorded and not send anything to the database
  
  %Test Case Derivation: \wss{Justify the expected value given in the Output field}
            
  How test will be performed: No tests will be performed and an empty table will be shown. Submit to database will be clicked \\

  \item{\bf{ST-DA 5}}
  
  Type: Manual
            
  Initial State: Device is on and is connected to a PC, the desktop application is open
            
  Input: User select live data viewing on the application
            
  Output: The application should start to receive live data from the device
  
  %Test Case Derivation: \wss{Justify the expected value given in the Output field}
            
  How test will be performed: The hardware device will be plugged into the PC and once we hit the live data button on the application all the sensors will display their output\\

\end{enumerate}

\subsubsection{Data Analytics Website}
\begin{enumerate}
  
  \item{\bf{ST-DAW 1}}
  
  Type: Manual
            
  Initial State: The database contains test data and is connected to the analytics platform
            
  Input: User logs into the analytics platform and selects a test
            
  Output: The test data is displayed showing the results over time for that particular test
  
  %Test Case Derivation: \wss{Justify the expected value given in the Output field}
            
  How test will be performed: We will populate the database with different tests and the user once logged in will select a specific test to view\\

  \item{\bf{ST-DAW 2}}
  
  Type: Manual
            
  Initial State: The database contains test data and is connected to the analytics platform
            
  Input: Unauthorized user attempts to login
            
  Output: The website alerts the user they are not authorized to view the dashboard
  
  %Test Case Derivation: \wss{Justify the expected value given in the Output field}
            
  How test will be performed: One team member will not have privileges to view the dashboard and will attempt to login\\

\end{enumerate}

\subsubsection{Database}

\begin{enumerate}
  
  \item{\bf{ST-D 1}}
  
  Type: Manual
            
  Initial State: Database is active
            
  Input: User User submits data multiple times within 5 seconds
            
  Output: Database stops accepting new data and alerts user
            
  How test will be performed: After a test is completed on the device, it will be previewed using the desktop app and the submit to database button will be pressed repeatedly\\
\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the
  appropriate functional tests from above.  The test cases should mention
  reporting the relative error for these tests.}

\wss{Tests related to usability could include conducting a usability test and
  survey.}

\subsubsection{Performance}
		
\paragraph{Operational in physical environment}

\begin{enumerate}

\item{Operational in physical enviornment\\}

Type: Dynamic, Manual
					
Initial State: Device is on and mounted to the device, has connected to the application and is waiting to start measuring.
					
Input/Condition: Vehicle's motor starts and values start to get picked up by device
					
Output/Result: Device is operational and stays physically intact in all types of weather and at 20\% greater than threshold values.
					
How test will be performed: The device will be tested outdoors under various weather conditions including rain, windy, etc.
The device will also be tested in temperature and vibration conditions that are above threshold values. This will be performed by placing the device in a hot environment
and vigoursly shaking it while being on a stationary mount.
					
\item{Viewing live data\\}

Type: Dynamic, Manual
					
Initial State: Device is on and mounted to the device, has connected to the application and is waiting to start measuring.
					
Input: Vehicle's motor starts and values start to get picked up by device
					
Output/Result: Data latency should be less than 30 seconds to simulate viewing live data.
					
How test will be performed: The amount of time for data to start being viewable on the application will be inspected to be less than 30 seconds.
The application will also be inspected to ensure that data is smooth and not lagging while measurements are being performed.

\item{Modularity and Maintainability\\}

Type: Dynamic, Manual
					
Initial State: Device is on measuring and sending values to the application, and connection to database has been verified
					
Input: Either the device, application, or database is disconnected or turned off
					
Output: The other two components are still functional even though communication between them is broken.
					
How test will be performed: While device, application, and database are fully functional and communicating successfully, different combinations of either one or two components
will be turned off. The other component(s) will be inspected to ensure that they are operational and indicating that the other component(s) are disconnected.

\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Usability}

\begin{enumerate}

\item{Mounting hardware and starting measurements\\}

Type: Dynamic, Manual
          
Initial State: Device is turned off and nothing is connected, only the application is loaded on to the computer
          
Input: Users will be asked to setup device and start taking measurements, rate setup process using a survey
          
Output: Time for setup and data to appear on the application should be less than 5 minutes and 
          
How test will be performed: A test group will be educated on the setup and connection of the device, then they will attempt to do that process. 
Each person will be timed and compared to the 5 minute threshold. In addition, they will be given a survey to rate the setup process on a scale from 
1 to 5 the following categories: ease of use, need for assistance,  

\item{test-id2\\}

Type: Dynamic, Manual
					
Initial State: Device is given to McMaster's Formula E team to use
					
Input: Using a survey, Formula E members will compare their current testing process to the Formulate process
					
Output: All users need to select Formulate in at least 2 of the 3 categories
					
How test will be performed: Formula E members will select which process is preferred in the following categories: speed, data collection, ease of use

\item{test-id2\\}

Type: Dynamic, Manual
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}
\begin{tabular}{| p{0.45\textwidth} | p{0.45\textwidth}|}
  \hline
  \rowcolor[gray]{0.9}
  Requirement & Test \\
  \hline
  FR 1 & ST-SV 1, ST-SV 2 \\
  \hline
  FR 2 & ST-DT 1, ST-DT 2, ST-DT 3, ST-DT 4 \\
  \hline
  FR 3 & ST-DT 1, ST-DT 2 \\
  \hline
  FR 4 & ST-DT 3, ST-DT 4 \\
  \hline
  FR 5 & ST-SV 1, ST-SV 2 \\
  \hline
  FR 6 & ST-DA 1 \\
  \hline
  FR 7 & ST-DA 3 ST-DA 4 \\
  \hline
  FR 8 & ST-DAW 1 \\
  \hline
  FR 9 & ST-DH 3 \\
  \hline
  FR 10 & ST-DH 4 \\
  \hline
  FR 11 & ST-DH 2 \\
  \hline
  FR 12 & ST-DT 1, ST-DT 2 \\
  \hline
  FR 13 &  \\
  \hline
  FR 14 &  \\
  \hline
  FR 15 & ST-DA 5\\
  \hline
  FR 16 &  \\
  \hline
  FR 17 & ST-DA 1 \\
  \hline
  FR 18 & ST-DA 2 \\
  \hline
  FR 19 & ST-DAW 2 \\
  \hline
  FR 20 & ST-DAW 1 \\
  \hline
  FR 21 & ST-DAW 1 \\
  \hline
  FR 22 & ST-D 1 \\
  \hline
  FR 23 &  \\
  \hline
  
\end{tabular}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}

\section{Unit Test Description}

\wss{Reference your MIS and explain your overall philosophy for test case
  selection.}  
\wss{This section should not be filled in until after the MIS has
  been completed.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}



\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\end{document}